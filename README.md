# Compiler-Construction
Compiler Construction involves the design and development of software tools that translate high-level programming languages into machine code or intermediate code. This intricate process encompasses lexical analysis, syntax analysis, semantic analysis, optimization, and code generation. By meticulously crafting compilers, programmers facilitate the seamless translation of human-readable code into executable programs, ensuring efficiency, portability, and compatibility across diverse computing platforms.

Lexical Analyzer / Token Generator:
The first step in compiler construction, the lexical analyzer (or token generator) breaks down the input source code into meaningful units called tokens. These tokens represent the smallest units of the language syntax, such as identifiers, keywords, constants, and operators. The lexical analyzer removes unnecessary whitespace and comments, producing a stream of tokens that serve as input for subsequent phases of the compiler.

Symbol Table:
A critical data structure in compiler design, the symbol table stores information about identifiers used in the source code, including their names, types, scopes, and memory locations. It facilitates efficient management and lookup of symbols during compilation, enabling the compiler to resolve identifiers and enforce semantic rules.

CFG (Context-Free Grammar):
Context-Free Grammar defines the syntax rules of a programming language using a formal notation. It consists of a set of production rules that specify how valid program constructs can be formed. CFG plays a crucial role in parsing, as it serves as the basis for constructing parsing algorithms that analyze the syntactic structure of the input source code.

Parsing Tree:
Also known as a syntax tree or abstract syntax tree (AST), the parsing tree is a hierarchical representation of the syntactic structure of the source code according to the rules defined by the CFG. Each node in the tree corresponds to a syntactic construct, with child nodes representing its components. Parsing trees are used in subsequent compilation phases for semantic analysis, optimization, and code generation.

RE (Regular Expressions):
Regular expressions are a powerful tool for describing patterns in strings. In compiler construction, regular expressions are used to define the lexical structure of tokens in a programming language. By specifying patterns for identifiers, keywords, operators, and other lexical elements, regular expressions enable the lexical analyzer to recognize and tokenize the input source code accurately.

Syntax Analyzer:
Also known as the parser, the syntax analyzer is responsible for analyzing the syntactic structure of the input source code according to the rules defined by the CFG. It verifies whether the sequence of tokens generated by the lexical analyzer conforms to the syntax rules of the programming language. The syntax analyzer produces a parsing tree or reports syntax errors if the source code violates the language syntax.

3 Address Code:
Three-address code is an intermediate representation of the source code that simplifies subsequent compilation phases, such as optimization and code generation. In 3-address code, each instruction contains at most three operands, facilitating efficient manipulation and analysis. It represents expressions in a form that is closer to the target machine's architecture, enabling the compiler to generate optimized and executable machine code.
